
# ğŸ‘‹ Automatic Hand Tracking with SAM 2 ğŸ–ï¸

This project implements automatic hand tracking in videos ğŸ¬ using MediaPipe for hand detection ğŸ” and Segment Anything Model 2 (SAM 2) for segmentation ğŸ§ .

## ğŸ› ï¸ Setup

1. Clone this repository: â¬‡ï¸
   ```bash
   git clone https://github.com/dpraj007/Automatic_hand_tracking.git
   cd Automatic_hand_tracking
   ```

2. Create a virtual environment (optional but recommended):  virtual environment isolates project dependencies ğŸ“¦
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Linux/macOS ğŸ
   venv\Scripts\activate  # On Windows ğŸªŸ
   ```

3. Install the required packages: âš™ï¸ dependencies listed in `requirements.txt`
   ```bash
   pip install -r requirements.txt
   ```

4. Download the SAM 2 checkpoint: ğŸ’¾ SAM model weights for segmentation
   ```bash
   wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
   ```

## ğŸš€ Usage

1. Place your input video file ğŸ“‚ in the project directory.

2. Open and run the Jupyter Notebook ğŸ““ `Automatic_Hand_Tracking_03.ipynb`. You can use Jupyter Lab or Google Colab â˜ï¸.

3. The processed video with hand masks âœ¨ will be saved as `output_final.mp4` in the same directory. ğŸ‰



## ğŸ“ Notes

- This script is optimized for GPU usage ğŸš€. If you're using Google Colab, it will automatically utilize the available GPU ğŸ’¡.
- The script processes the entire video frame by frame â³, which may take some time depending on the video length and your hardware ğŸ’».
- Adjust the `max_num_hands` and `min_detection_confidence` parameters in the `detect_hands` function within the notebook ğŸ““ if needed to fine-tune hand detection.

## ğŸ“œ License

This project is licensed under the MIT License âš–ï¸ - see the LICENSE file for details. ğŸ“„

